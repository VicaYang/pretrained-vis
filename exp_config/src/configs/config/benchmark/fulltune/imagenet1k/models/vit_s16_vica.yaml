# @package _global_
config:
  DATA:
    TRAIN:
      BATCHSIZE_PER_REPLICA: 32 # Fits on 16gb GPU
    TEST:
      BATCHSIZE_PER_REPLICA: 32
  MODEL:
    TRUNK: # S-16
      NAME: vision_transformer
      VISION_TRANSFORMERS:
        IMAGE_SIZE: 224
        PATCH_SIZE: 16
        HIDDEN_DIM: 384
        NUM_LAYERS: 12
        NUM_HEADS: 12
        MLP_DIM: 1536
        CLASSIFIER: token
        DROPOUT_RATE: 0
        ATTENTION_DROPOUT_RATE: 0
        QKV_BIAS: True
        DROP_PATH_RATE: 0.1 # stochastic depth dropout probability
    HEAD:
      PARAMS: [
        ["mlp", {"dims": [384, 1000]}],
      ]
    WEIGHTS_INIT:
      PARAMS_FILE: "specify the model weights"
      STATE_DICT_KEY_NAME: classy_state_dict
      SKIP_LAYERS: [
        'heads.0.clf.0.weight',
        'heads.0.clf.0.bias',
        'num_batches_tracked'
      ]
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: False
      SYNC_BN_TYPE: apex
      GROUP_SIZE: 8
    AMP_PARAMS:
      USE_AMP: True
      AMP_ARGS: {"opt_level": "O1"}
